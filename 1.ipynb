{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔科夫决策过程（MDP）笔记\n",
    "\n",
    "## 1. 马尔科夫决策过程（MDP）简介\n",
    "马尔科夫决策过程（MDP）是一个用于建模序列决策的数学框架，特别适用于强化学习（RL）。MDP由以下几个部分组成：\n",
    "\n",
    "- **状态空间 (S)**: 所有可能状态的集合。\n",
    "- **动作空间 (A)**: 所有可能动作的集合。\n",
    "- **状态转移概率 (P)**: 从当前状态转移到下一个状态的概率，通常表示为 \\( P(s'|s, a) \\)，即在状态 \\(s\\) 下采取动作 \\(a\\) 后，转移到状态 \\(s'\\) 的概率。\n",
    "- **回报函数 (R)**: 给定状态和动作，系统会获得的即时回报 \\( R(s, a) \\)。\n",
    "- **折扣因子 (γ)**: 介于0和1之间的数，表示未来回报的折扣程度，通常用于平衡即时回报与未来回报的权重。\n",
    "\n",
    "### 1.1 MDP的核心思想\n",
    "MDP的目标是通过学习一个策略 \\( \\pi(a|s) \\) 来最大化长期的累积回报，策略定义了在每个状态下应该采取什么动作。\n",
    "\n",
    "## 2. 回报与折扣回报\n",
    "### 2.1 回报（Reward）\n",
    "回报是系统在某一时刻获得的即时奖励。给定当前状态 \\(s_t\\) 和所选动作 \\(a_t\\)，可以获得即时回报 \\(r_t = R(s_t, a_t)\\)。\n",
    "\n",
    "### 2.2 折扣回报（Discounted Return）\n",
    "折扣回报是对未来回报的折扣处理，表示为：\n",
    "$$\n",
    "G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}\n",
    "$$\n",
    "其中，\\( \\gamma \\) 是折扣因子，表示未来回报的重要性，\\( 0 \\leq \\gamma \\leq 1 \\)。\n",
    "\n",
    "### 2.3 长期回报的目标\n",
    "强化学习的目标是最大化从当前状态开始的折扣回报：\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} \\mid s_t = s, \\pi \\right]\n",
    "$$\n",
    "其中，\\( \\pi \\) 是策略，\\( V^\\pi(s) \\) 是在状态 \\(s\\) 下按照策略 \\( \\pi \\) 采取行动的长期回报。\n",
    "\n",
    "## 3. 价值函数\n",
    "### 3.1 状态价值函数 (State Value Function)\n",
    "状态价值函数 \\(V^\\pi(s)\\) 表示在状态 \\(s\\) 下，按照策略 \\( \\pi \\) 行动所能获得的预期回报：\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_t = s, \\pi \\right]\n",
    "$$\n",
    "\n",
    "### 3.2 动作价值函数 (Action-Value Function)\n",
    "动作价值函数 \\(Q^\\pi(s, a)\\) 表示在状态 \\(s\\) 下采取动作 \\(a\\)，然后按照策略 \\( \\pi \\) 行动所能获得的预期回报：\n",
    "$$\n",
    "Q^\\pi(s, a) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_t = s, a_t = a, \\pi \\right]\n",
    "$$\n",
    "\n",
    "## 4. 最优价值函数与最优策略\n",
    "### 4.1 最优状态价值函数 (Optimal State Value Function)\n",
    "最优状态价值函数 \\(V^*(s)\\) 是指在状态 \\(s\\) 下，能够获得最大长期回报的价值函数。它满足：\n",
    "$$\n",
    "V^*(s) = \\max_\\pi V^\\pi(s)\n",
    "$$\n",
    "\n",
    "### 4.2 最优动作价值函数 (Optimal Action-Value Function)\n",
    "最优动作价值函数 \\(Q^*(s, a)\\) 是指在状态 \\(s\\) 下采取动作 \\(a\\)，然后遵循最优策略所能获得的最大预期回报：\n",
    "$$\n",
    "Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)\n",
    "$$\n",
    "\n",
    "### 4.3 最优策略 (Optimal Policy)\n",
    "最优策略 \\( \\pi^*(s) \\) 是在状态 \\(s\\) 下能够最大化长期回报的策略，即：\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a Q^*(s, a)\n",
    "$$\n",
    "\n",
    "## 5. 价值学习与策略学习\n",
    "### 5.1 价值学习 (Value Iteration)\n",
    "价值学习是通过不断更新状态的价值函数来逼近最优价值函数的方法。常用的算法是**值迭代算法**，通过贝尔曼方程更新状态的价值：\n",
    "$$\n",
    "V_{t+1}(s) = \\max_a \\left( R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_t(s') \\right)\n",
    "$$\n",
    "\n",
    "### 5.2 策略学习 (Policy Iteration)\n",
    "策略学习是通过不断评估和改进策略来逼近最优策略的方法。常用的算法是**策略迭代**，包含两个步骤：\n",
    "1. **策略评估**: 对当前策略进行评估，得到状态价值函数 \\(V^\\pi(s)\\)。\n",
    "2. **策略改进**: 更新策略，选择能最大化动作价值函数的动作：\n",
    "$$\n",
    "\\pi_{t+1}(s) = \\arg\\max_a Q^\\pi(s, a)\n",
    "$$\n",
    "\n",
    "### 5.3 Q-learning（Q学习）\n",
    "Q-learning 是一种无模型的强化学习算法，它通过不断更新动作价值函数 \\( Q(s, a) \\) 来学习最优策略。更新公式为：\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_a Q(s', a) - Q(s, a) \\right)\n",
    "$$\n",
    "其中， $ \\alpha $ 是学习率。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
